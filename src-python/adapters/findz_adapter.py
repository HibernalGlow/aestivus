"""
findz é€‚é…å™¨
æ–‡ä»¶æœç´¢å·¥å…· - ä½¿ç”¨ SQL-like WHERE è¯­æ³•æœç´¢æ–‡ä»¶ï¼ˆåŒ…æ‹¬å‹ç¼©åŒ…å†…éƒ¨ï¼‰

æ”¯æŒåŠŸèƒ½ï¼š
1. search: ä½¿ç”¨ WHERE è¯­æ³•æœç´¢æ–‡ä»¶
2. nested: æŸ¥æ‰¾åŒ…å«åµŒå¥—å‹ç¼©åŒ…çš„å¤–å±‚å‹ç¼©åŒ…
3. archives_only: åªæœç´¢å‹ç¼©åŒ…æœ¬èº«
"""

import io
import os
import sys
import traceback
from datetime import datetime
from pathlib import Path
from typing import Callable, Dict, List, Optional, Any

from pydantic import Field

from .base import BaseAdapter, AdapterInput, AdapterOutput


def _ensure_utf8_output():
    """ç¡®ä¿ stdout/stderr ä½¿ç”¨ UTF-8 ç¼–ç """
    if sys.platform == 'win32':
        os.environ.setdefault('PYTHONIOENCODING', 'utf-8')
        if hasattr(sys.stdout, 'buffer'):
            sys.stdout = io.TextIOWrapper(
                sys.stdout.buffer, 
                encoding='utf-8', 
                errors='replace',
                line_buffering=True
            )
        if hasattr(sys.stderr, 'buffer'):
            sys.stderr = io.TextIOWrapper(
                sys.stderr.buffer, 
                encoding='utf-8', 
                errors='replace',
                line_buffering=True
            )


_ensure_utf8_output()


class FindzInput(AdapterInput):
    """findz è¾“å…¥å‚æ•°"""
    path: str = Field(default="", description="æœç´¢è·¯å¾„")
    paths: List[str] = Field(default_factory=list, description="æœç´¢è·¯å¾„åˆ—è¡¨")
    where: str = Field(default="1", description="WHERE è¿‡æ»¤è¡¨è¾¾å¼ (SQL æˆ– JSON)")
    filter_config: Optional[Dict] = Field(default=None, description="JSON æ ¼å¼çš„è¿‡æ»¤å™¨é…ç½®")
    filter_mode: str = Field(default="auto", description="è¿‡æ»¤å™¨æ¨¡å¼: auto/sql/json")
    action: str = Field(default="search", description="æ“ä½œç±»å‹: search/nested/archives_only/interactive")
    long_format: bool = Field(default=True, description="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆæ—¥æœŸã€å¤§å°ï¼‰")
    follow_symlinks: bool = Field(default=False, description="è·Ÿéšç¬¦å·é“¾æ¥")
    no_archive: bool = Field(default=False, description="ç¦ç”¨å‹ç¼©åŒ…æœç´¢")
    max_results: int = Field(default=0, description="æœ€å¤§ç»“æœæ•°é‡ï¼Œ0è¡¨ç¤ºæ— é™åˆ¶")
    max_return_files: int = Field(default=5000, description="æœ€å¤§è¿”å›æ–‡ä»¶æ•°ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰ï¼Œ0è¡¨ç¤ºå…¨éƒ¨è¿”å›")
    continue_on_error: bool = Field(default=True, description="é‡åˆ°é”™è¯¯ç»§ç»­æœç´¢")


class FindzOutput(AdapterOutput):
    """findz è¾“å‡ºç»“æœ"""
    # æœç´¢ç»“æœ
    total_count: int = Field(default=0, description="æ€»æ–‡ä»¶æ•°")
    file_count: int = Field(default=0, description="æ™®é€šæ–‡ä»¶æ•°")
    dir_count: int = Field(default=0, description="ç›®å½•æ•°")
    archive_count: int = Field(default=0, description="å‹ç¼©åŒ…æ•°")
    nested_count: int = Field(default=0, description="åµŒå¥—å‹ç¼©åŒ…æ•°")
    # æ–‡ä»¶åˆ—è¡¨
    files: List[Dict[str, Any]] = Field(default_factory=list, description="æ–‡ä»¶åˆ—è¡¨")
    # æŒ‰ç±»å‹åˆ†ç»„
    by_extension: Dict[str, int] = Field(default_factory=dict, description="æŒ‰æ‰©å±•åç»Ÿè®¡")
    by_archive: Dict[str, int] = Field(default_factory=dict, description="æŒ‰å‹ç¼©åŒ…ç»Ÿè®¡")
    # é”™è¯¯ä¿¡æ¯
    errors: List[str] = Field(default_factory=list, description="é”™è¯¯åˆ—è¡¨")


class FindzAdapter(BaseAdapter):
    """findz é€‚é…å™¨ - æ–‡ä»¶æœç´¢å·¥å…·"""
    
    name = "findz"
    display_name = "æ–‡ä»¶æœç´¢"
    description = "ä½¿ç”¨ SQL-like WHERE è¯­æ³•æˆ–å¯è§†åŒ–é…ç½®æœç´¢æ–‡ä»¶ï¼ˆæ”¯æŒå‹ç¼©åŒ…å†…éƒ¨ï¼‰"
    category = "file"
    icon = "ğŸ”"
    required_packages = ["findz"]
    input_schema = FindzInput
    output_schema = FindzOutput
    
    def _import_module(self) -> Dict:
        """æ‡’åŠ è½½å¯¼å…¥ findz æ¨¡å—"""
        from findz.filter.filter import create_filter
        from findz.filter.size import format_size
        from findz.find.find import FileInfo, FIELDS
        from findz.find.walk import WalkParams, walk, is_archive
        from findz.find.index_cache import get_global_cache
        
        # å°è¯•å¯¼å…¥ç»Ÿä¸€è¿‡æ»¤å™¨ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰
        try:
            from findz.filter.unified import create_unified_filter
        except ImportError:
            create_unified_filter = None
        
        return {
            'create_filter': create_filter,
            'create_unified_filter': create_unified_filter,
            'format_size': format_size,
            'FileInfo': FileInfo,
            'FIELDS': FIELDS,
            'WalkParams': WalkParams,
            'walk': walk,
            'is_archive': is_archive,
            'get_global_cache': get_global_cache,
        }
    
    def _create_filter(self, input_data: FindzInput, module: Dict):
        """
        åˆ›å»ºè¿‡æ»¤å™¨è¡¨è¾¾å¼
        æ”¯æŒ SQL å­—ç¬¦ä¸²å’Œ JSON é…ç½®ä¸¤ç§æ¨¡å¼
        """
        create_unified_filter = module.get('create_unified_filter')
        create_filter = module['create_filter']
        
        # ä¼˜å…ˆä½¿ç”¨ JSON é…ç½®
        if input_data.filter_config and create_unified_filter:
            return create_unified_filter(input_data.filter_config, mode='json')
        
        # ä½¿ç”¨ç»Ÿä¸€è¿‡æ»¤å™¨ï¼ˆè‡ªåŠ¨æ£€æµ‹æ¨¡å¼ï¼‰
        if create_unified_filter:
            return create_unified_filter(
                input_data.where or "1",
                mode=input_data.filter_mode
            )
        
        # å›é€€åˆ°åŸå§‹ SQL è¿‡æ»¤å™¨
        return create_filter(input_data.where or "1")
    
    async def execute(
        self,
        input_data: FindzInput,
        on_progress: Optional[Callable[[int, str], None]] = None,
        on_log: Optional[Callable[[str], None]] = None
    ) -> FindzOutput:
        """æ‰§è¡Œ findz æ“ä½œ"""
        action = input_data.action.lower()
        
        if action == "search":
            return await self._search(input_data, on_progress, on_log)
        elif action == "nested":
            return await self._find_nested(input_data, on_progress, on_log)
        elif action == "archives_only":
            return await self._archives_only(input_data, on_progress, on_log)
        elif action == "interactive":
            return await self._interactive_help(input_data, on_progress, on_log)
        else:
            return FindzOutput(success=False, message=f"æœªçŸ¥æ“ä½œ: {action}")
    
    def _collect_paths(self, input_data: FindzInput) -> List[str]:
        """æ”¶é›†å¹¶éªŒè¯è·¯å¾„"""
        paths = list(input_data.paths) if input_data.paths else []
        if input_data.path:
            path = input_data.path.strip().strip('"')
            if path and path not in paths:
                paths.append(path)
        # å»é™¤å¼•å·å¹¶éªŒè¯å­˜åœ¨
        valid_paths = []
        for p in paths:
            p = p.strip().strip('"')
            if Path(p).exists():
                valid_paths.append(p)
        return valid_paths if valid_paths else ["."]
    
    def _file_info_to_dict(self, file_info, format_size) -> Dict[str, Any]:
        """å°† FileInfo è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'name': file_info.name,
            'path': file_info.path,
            'size': file_info.size,
            'size_formatted': format_size(file_info.size),
            'mod_time': file_info.mod_time.isoformat(),
            'date': file_info.mod_time.strftime("%Y-%m-%d"),
            'time': file_info.mod_time.strftime("%H:%M:%S"),
            'type': file_info.file_type,
            'container': file_info.container or '',
            'archive': file_info.archive or '',
            'ext': os.path.splitext(file_info.name)[1].lstrip('.'),
        }

    async def _search(
        self,
        input_data: FindzInput,
        on_progress: Optional[Callable[[int, str], None]] = None,
        on_log: Optional[Callable[[str], None]] = None
    ) -> FindzOutput:
        """
        æ‰§è¡Œæ–‡ä»¶æœç´¢
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. æ‰¹é‡å¤„ç†ï¼šæ¯ BATCH_SIZE ä¸ªæ–‡ä»¶å¤„ç†ä¸€æ¬¡ï¼Œå‡å°‘ UI æ›´æ–°é¢‘ç‡
        2. å®æ—¶è¿›åº¦ï¼šåŸºäºæ‰«ææ–‡ä»¶æ•°åŠ¨æ€è®¡ç®—è¿›åº¦
        3. å¼‚æ­¥è®©å‡ºï¼šå®šæœŸè®©å‡ºæ§åˆ¶æƒï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        """
        import asyncio
        import time
        
        # æ€§èƒ½é…ç½®
        BATCH_SIZE = 500  # æ¯æ‰¹å¤„ç†æ–‡ä»¶æ•°
        PROGRESS_INTERVAL = 0.1  # è¿›åº¦æ›´æ–°æœ€å°é—´éš”ï¼ˆç§’ï¼‰
        YIELD_INTERVAL = 1000  # æ¯å¤„ç†å¤šå°‘æ–‡ä»¶è®©å‡ºä¸€æ¬¡æ§åˆ¶æƒ
        
        paths = self._collect_paths(input_data)
        where = input_data.where or "1"
        
        try:
            if on_log:
                on_log(f"ğŸ“¦ å¯¼å…¥ findz æ¨¡å—...")
            
            module = self._import_module()
            format_size = module['format_size']
            WalkParams = module['WalkParams']
            walk = module['walk']
            is_archive = module['is_archive']
            get_global_cache = module['get_global_cache']
            
            if on_log:
                on_log(f"ğŸ” å¼€å§‹æœç´¢: {', '.join(paths)}")
                if input_data.filter_config:
                    on_log(f"ğŸ“ è¿‡æ»¤é…ç½®: JSON æ¨¡å¼")
                else:
                    on_log(f"ğŸ“ è¿‡æ»¤æ¡ä»¶: {where}")
                on_log(f"âš™ï¸ é€‰é¡¹: no_archive={input_data.no_archive}, max_results={input_data.max_results}")
            
            if on_progress:
                on_progress(5, "è§£æè¿‡æ»¤å™¨...")
            
            # åˆ›å»ºè¿‡æ»¤å™¨ï¼ˆæ”¯æŒ SQL å’Œ JSON ä¸¤ç§æ¨¡å¼ï¼‰
            try:
                if on_log:
                    on_log(f"ğŸ”§ åˆ›å»ºè¿‡æ»¤å™¨...")
                filter_expr = self._create_filter(input_data, module)
                if on_log:
                    on_log(f"âœ… è¿‡æ»¤å™¨åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                if on_log:
                    on_log(f"âŒ è¿‡æ»¤å™¨è¯­æ³•é”™è¯¯: {e}")
                return FindzOutput(success=False, message=f"è¿‡æ»¤å™¨è¯­æ³•é”™è¯¯: {e}")
            
            # é”™è¯¯æ”¶é›†
            errors = []
            def error_handler(msg: str) -> None:
                if len(errors) < 100:  # é™åˆ¶é”™è¯¯æ•°é‡
                    errors.append(msg)
                if not input_data.continue_on_error:
                    raise RuntimeError(msg)
            
            if on_progress:
                on_progress(10, "æœç´¢æ–‡ä»¶...")
            
            # æ‰§è¡Œæœç´¢
            all_results = []
            by_extension: Dict[str, int] = {}
            by_archive: Dict[str, int] = {}
            file_count = 0
            dir_count = 0
            archive_count = 0
            scanned_files = 0
            scanned_archives = 0
            
            # è¿›åº¦æ§åˆ¶
            last_progress_time = time.time()
            last_log_count = 0
            start_time = time.time()
            
            for search_path in paths:
                if on_log:
                    on_log(f"ğŸ“‚ æ‰«æç›®å½•: {search_path}")
                
                params = WalkParams(
                    filter_expr=filter_expr,
                    follow_symlinks=input_data.follow_symlinks,
                    no_archive=input_data.no_archive,
                    archives_only=False,
                    use_cache=True,
                    max_workers=4,
                    error_handler=error_handler,
                )
                
                try:
                    for file_info in walk(search_path, params):
                        scanned_files += 1
                        
                        # å®šæœŸè®©å‡ºæ§åˆ¶æƒï¼Œé¿å…é˜»å¡
                        if scanned_files % YIELD_INTERVAL == 0:
                            await asyncio.sleep(0)
                        
                        # å®æ—¶è¿›åº¦æ›´æ–°ï¼ˆåŸºäºæ—¶é—´é—´éš”ï¼‰
                        current_time = time.time()
                        if current_time - last_progress_time >= PROGRESS_INTERVAL:
                            # åŠ¨æ€è®¡ç®—è¿›åº¦ï¼ˆ10-90%ï¼‰
                            elapsed = current_time - start_time
                            # åŸºäºæ‰«æé€Ÿåº¦ä¼°ç®—è¿›åº¦
                            progress = min(10 + int(80 * (1 - 1 / (1 + scanned_files / 10000))), 90)
                            
                            if on_progress:
                                on_progress(progress, f"æ‰«æä¸­: {scanned_files} æ–‡ä»¶, {len(all_results)} åŒ¹é…")
                            
                            last_progress_time = current_time
                        
                        # æ‰¹é‡æ—¥å¿—ï¼ˆæ¯ BATCH_SIZE ä¸ªæ–‡ä»¶è®°å½•ä¸€æ¬¡ï¼‰
                        if scanned_files - last_log_count >= BATCH_SIZE:
                            if on_log:
                                speed = scanned_files / max(current_time - start_time, 0.1)
                                on_log(f"ğŸ“Š å·²æ‰«æ {scanned_files} æ–‡ä»¶ ({speed:.0f}/s)ï¼Œæ‰¾åˆ° {len(all_results)} åŒ¹é…")
                            last_log_count = scanned_files
                        
                        # é™åˆ¶ç»“æœæ•°é‡ï¼ˆ0è¡¨ç¤ºæ— é™åˆ¶ï¼‰
                        if input_data.max_results > 0 and len(all_results) >= input_data.max_results:
                            if on_log:
                                on_log(f"âš ï¸ ç»“æœå·²è¾¾ä¸Šé™ {input_data.max_results}")
                            break
                        
                        # è½¬æ¢ä¸ºå­—å…¸
                        file_dict = self._file_info_to_dict(file_info, format_size)
                        all_results.append(file_dict)
                        
                        # ç»Ÿè®¡
                        ext = file_dict['ext'].lower()
                        by_extension[ext] = by_extension.get(ext, 0) + 1
                        
                        if file_info.archive:
                            by_archive[file_info.archive] = by_archive.get(file_info.archive, 0) + 1
                            archive_count += 1
                            if file_info.archive not in by_archive or by_archive[file_info.archive] == 1:
                                scanned_archives += 1
                        
                        if file_info.file_type == 'dir':
                            dir_count += 1
                        else:
                            file_count += 1
                    
                except Exception as e:
                    if on_log:
                        on_log(f"âŒ æœç´¢å¼‚å¸¸: {type(e).__name__}: {e}")
                    if input_data.continue_on_error:
                        errors.append(f"{search_path}: {e}")
                    else:
                        return FindzOutput(success=False, message=f"æœç´¢å¤±è´¥: {e}")
            
            # ä¿å­˜ç¼“å­˜
            if on_log:
                on_log(f"ğŸ’¾ ä¿å­˜ç¼“å­˜...")
            cache = get_global_cache()
            cache.flush()
            
            if on_progress:
                on_progress(100, "æœç´¢å®Œæˆ")
            
            # è®¡ç®—æ€»è€—æ—¶
            total_time = time.time() - start_time
            
            if on_log:
                on_log(f"âœ… æœç´¢å®Œæˆ: æ‰«æ {scanned_files} æ–‡ä»¶ï¼Œ{scanned_archives} å‹ç¼©åŒ…ï¼Œè€—æ—¶ {total_time:.1f}s")
                on_log(f"ğŸ“Š æ‰¾åˆ° {len(all_results)} åŒ¹é… (æ–‡ä»¶:{file_count}, ç›®å½•:{dir_count}, å‹ç¼©åŒ…å†…:{archive_count})")
                if errors:
                    on_log(f"âš ï¸ {len(errors)} ä¸ªé”™è¯¯")
            
            # é™åˆ¶è¿”å›çš„æ–‡ä»¶æ•°é‡ï¼ˆé¿å…å‰ç«¯å¡é¡¿ï¼‰
            max_return = input_data.max_return_files
            return_files = all_results[:max_return] if max_return > 0 else all_results
            truncated = len(all_results) > len(return_files)
            
            if truncated and on_log:
                on_log(f"ğŸ“‹ è¿”å›å‰ {len(return_files)} æ¡è®°å½•ï¼ˆå…± {len(all_results)} æ¡ï¼‰")
            
            return FindzOutput(
                success=True,
                message=f"æ‰¾åˆ° {len(all_results)} ä¸ªæ–‡ä»¶ ({total_time:.1f}s)",
                total_count=len(all_results),
                file_count=file_count,
                dir_count=dir_count,
                archive_count=archive_count,
                files=return_files,  # åªè¿”å›éƒ¨åˆ†æ–‡ä»¶
                by_extension=by_extension,
                by_archive=by_archive,
                errors=errors[:20],
                data={
                    'total_count': len(all_results),
                    'file_count': file_count,
                    'dir_count': dir_count,
                    'archive_count': archive_count,
                    'files': return_files,  # åªè¿”å›éƒ¨åˆ†æ–‡ä»¶
                    'by_extension': by_extension,
                    'by_archive': by_archive,
                    'errors': errors[:20],
                    'paths': paths,
                    'where': where,
                    'scanned_files': scanned_files,
                    'elapsed_time': total_time,
                    'truncated': truncated,
                    'returned_count': len(return_files),
                }
            )
            
        except ImportError as e:
            if on_log:
                on_log(f"âŒ findz æ¨¡å—æœªå®‰è£…: {e}")
            return FindzOutput(success=False, message=f"findz æ¨¡å—æœªå®‰è£…: {e}")
        except Exception as e:
            import traceback
            if on_log:
                on_log(f"âŒ æœç´¢å¤±è´¥: {type(e).__name__}: {e}")
                on_log(f"ğŸ“‹ å †æ ˆ: {traceback.format_exc()[:500]}")
            return FindzOutput(success=False, message=f"æœç´¢å¤±è´¥: {type(e).__name__}: {e}")
    
    async def _find_nested(
        self,
        input_data: FindzInput,
        on_progress: Optional[Callable[[int, str], None]] = None,
        on_log: Optional[Callable[[str], None]] = None
    ) -> FindzOutput:
        """æŸ¥æ‰¾åŒ…å«åµŒå¥—å‹ç¼©åŒ…çš„å¤–å±‚å‹ç¼©åŒ…"""
        paths = self._collect_paths(input_data)
        
        try:
            module = self._import_module()
            create_filter = module['create_filter']
            format_size = module['format_size']
            WalkParams = module['WalkParams']
            walk = module['walk']
            is_archive = module['is_archive']
            get_global_cache = module['get_global_cache']
            
            if on_log:
                on_log(f"ğŸ” æœç´¢åµŒå¥—å‹ç¼©åŒ…: {', '.join(paths)}")
            if on_progress:
                on_progress(10, "æœç´¢åµŒå¥—å‹ç¼©åŒ…...")
            
            # åˆ›å»ºåŒ¹é…æ‰€æœ‰æ–‡ä»¶çš„è¿‡æ»¤å™¨
            filter_expr = create_filter("1")
            
            # é”™è¯¯æ”¶é›†
            errors = []
            def error_handler(msg: str) -> None:
                if input_data.continue_on_error:
                    errors.append(msg)
            
            # æ”¶é›†åŒ…å«åµŒå¥—å‹ç¼©åŒ…çš„å¤–å±‚å‹ç¼©åŒ…
            nested_containers = set()
            
            for search_path in paths:
                params = WalkParams(
                    filter_expr=filter_expr,
                    follow_symlinks=input_data.follow_symlinks,
                    no_archive=False,  # å¿…é¡»æ‰«æå‹ç¼©åŒ…å†…éƒ¨
                    archives_only=False,
                    use_cache=True,
                    max_workers=4,
                    error_handler=error_handler,
                )
                
                try:
                    for file_info in walk(search_path, params):
                        # æ£€æŸ¥æ˜¯å¦åœ¨å‹ç¼©åŒ…å†…ï¼ˆarchive ä¸ä¸ºç©ºï¼‰
                        if file_info.archive:
                            # æ£€æŸ¥æ–‡ä»¶æœ¬èº«æ˜¯å¦æ˜¯å‹ç¼©åŒ…
                            if is_archive(file_info.name):
                                nested_containers.add(file_info.archive)
                except Exception as e:
                    if input_data.continue_on_error:
                        errors.append(f"{search_path}: {e}")
            
            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
            result_archives = sorted(nested_containers)
            
            # æ„å»ºç»“æœ
            files = []
            for archive_path in result_archives:
                if os.path.exists(archive_path):
                    try:
                        stat = os.stat(archive_path)
                        files.append({
                            'name': os.path.basename(archive_path),
                            'path': archive_path,
                            'size': stat.st_size,
                            'size_formatted': format_size(stat.st_size),
                            'mod_time': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                            'date': datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d"),
                            'time': datetime.fromtimestamp(stat.st_mtime).strftime("%H:%M:%S"),
                            'type': 'file',
                            'container': '',
                            'archive': '',
                            'ext': os.path.splitext(archive_path)[1].lstrip('.'),
                            'has_nested': True,
                        })
                    except Exception:
                        files.append({
                            'name': os.path.basename(archive_path),
                            'path': archive_path,
                            'size': 0,
                            'size_formatted': '0',
                            'type': 'file',
                            'has_nested': True,
                        })
            
            # ä¿å­˜ç¼“å­˜
            cache = get_global_cache()
            cache.flush()
            
            if on_progress:
                on_progress(100, "æœç´¢å®Œæˆ")
            
            if on_log:
                on_log(f"âœ… æ‰¾åˆ° {len(files)} ä¸ªåŒ…å«åµŒå¥—å‹ç¼©åŒ…çš„å¤–å±‚å‹ç¼©åŒ…")
            
            return FindzOutput(
                success=True,
                message=f"æ‰¾åˆ° {len(files)} ä¸ªåŒ…å«åµŒå¥—å‹ç¼©åŒ…çš„å¤–å±‚å‹ç¼©åŒ…",
                total_count=len(files),
                nested_count=len(files),
                files=files,
                errors=errors[:20],
                data={
                    'nested_count': len(files),
                    'files': files,
                    'errors': errors[:20],
                    'paths': paths,
                }
            )
            
        except ImportError as e:
            return FindzOutput(success=False, message=f"findz æ¨¡å—æœªå®‰è£…: {e}")
        except Exception as e:
            if on_log:
                on_log(f"âŒ æœç´¢å¤±è´¥: {e}")
            return FindzOutput(success=False, message=f"æœç´¢å¤±è´¥: {type(e).__name__}: {e}")

    async def _archives_only(
        self,
        input_data: FindzInput,
        on_progress: Optional[Callable[[int, str], None]] = None,
        on_log: Optional[Callable[[str], None]] = None
    ) -> FindzOutput:
        """åªæœç´¢å‹ç¼©åŒ…æœ¬èº«"""
        paths = self._collect_paths(input_data)
        where = input_data.where or "1"
        
        try:
            module = self._import_module()
            create_filter = module['create_filter']
            format_size = module['format_size']
            WalkParams = module['WalkParams']
            walk = module['walk']
            get_global_cache = module['get_global_cache']
            
            if on_log:
                on_log(f"ğŸ” æœç´¢å‹ç¼©åŒ…: {', '.join(paths)}")
                on_log(f"ğŸ“ è¿‡æ»¤æ¡ä»¶: {where}")
            if on_progress:
                on_progress(10, "æœç´¢å‹ç¼©åŒ…...")
            
            # åˆ›å»ºè¿‡æ»¤å™¨
            try:
                filter_expr = create_filter(where)
            except Exception as e:
                return FindzOutput(success=False, message=f"è¿‡æ»¤å™¨è¯­æ³•é”™è¯¯: {e}")
            
            # é”™è¯¯æ”¶é›†
            errors = []
            def error_handler(msg: str) -> None:
                if input_data.continue_on_error:
                    errors.append(msg)
            
            # æ‰§è¡Œæœç´¢
            all_results = []
            by_extension: Dict[str, int] = {}
            
            for search_path in paths:
                params = WalkParams(
                    filter_expr=filter_expr,
                    follow_symlinks=input_data.follow_symlinks,
                    no_archive=True,  # ä¸è¿›å…¥å‹ç¼©åŒ…å†…éƒ¨
                    archives_only=True,  # åªæœç´¢å‹ç¼©åŒ…
                    use_cache=True,
                    max_workers=4,
                    error_handler=error_handler,
                )
                
                try:
                    for file_info in walk(search_path, params):
                        # é™åˆ¶ç»“æœæ•°é‡ï¼ˆ0è¡¨ç¤ºæ— é™åˆ¶ï¼‰
                        if input_data.max_results > 0 and len(all_results) >= input_data.max_results:
                            if on_log:
                                on_log(f"âš ï¸ ç»“æœå·²è¾¾ä¸Šé™ {input_data.max_results}")
                            break
                        
                        file_dict = self._file_info_to_dict(file_info, format_size)
                        all_results.append(file_dict)
                        
                        ext = file_dict['ext'].lower()
                        by_extension[ext] = by_extension.get(ext, 0) + 1
                        
                except Exception as e:
                    if input_data.continue_on_error:
                        errors.append(f"{search_path}: {e}")
            
            if on_progress:
                on_progress(100, "æœç´¢å®Œæˆ")
            
            if on_log:
                on_log(f"âœ… æ‰¾åˆ° {len(all_results)} ä¸ªå‹ç¼©åŒ…")
            
            return FindzOutput(
                success=True,
                message=f"æ‰¾åˆ° {len(all_results)} ä¸ªå‹ç¼©åŒ…",
                total_count=len(all_results),
                archive_count=len(all_results),
                files=all_results,
                by_extension=by_extension,
                errors=errors[:20],
                data={
                    'archive_count': len(all_results),
                    'files': all_results,
                    'by_extension': by_extension,
                    'errors': errors[:20],
                    'paths': paths,
                    'where': where,
                }
            )
            
        except ImportError as e:
            return FindzOutput(success=False, message=f"findz æ¨¡å—æœªå®‰è£…: {e}")
        except Exception as e:
            if on_log:
                on_log(f"âŒ æœç´¢å¤±è´¥: {e}")
            return FindzOutput(success=False, message=f"æœç´¢å¤±è´¥: {type(e).__name__}: {e}")
    
    async def _interactive_help(
        self,
        input_data: FindzInput,
        on_progress: Optional[Callable[[int, str], None]] = None,
        on_log: Optional[Callable[[str], None]] = None
    ) -> FindzOutput:
        """è¿”å›äº¤äº’å¼å¸®åŠ©ä¿¡æ¯"""
        help_text = """
# findz è¿‡æ»¤è¯­æ³•

findz ä½¿ç”¨ç±»ä¼¼ SQL WHERE å­å¥çš„è¯­æ³•è¿›è¡Œæ–‡ä»¶æœç´¢ã€‚

## ç¤ºä¾‹

```
# æŸ¥æ‰¾å°äº 10KB çš„æ–‡ä»¶
size < 10k

# æŸ¥æ‰¾å¤§å°åœ¨ 1M åˆ° 1G ä¹‹é—´çš„æ–‡ä»¶
size between 1M and 1G

# æŸ¥æ‰¾ 2010 å¹´ä¹‹å‰ä¿®æ”¹çš„å‹ç¼©åŒ…å†…æ–‡ä»¶
date < "2010" and archive <> ""

# æŸ¥æ‰¾åä¸º foo* ä¸”ä»Šå¤©ä¿®æ”¹çš„æ–‡ä»¶
name like "foo%" and date = today

# ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾
name rlike "(.*-){2}"

# æŒ‰æ‰©å±•åæŸ¥æ‰¾
ext in ("jpg", "jpeg", "png")

# æŸ¥æ‰¾ç›®å½•
type = "dir"
```

## æ–‡ä»¶å±æ€§

- **name** - æ–‡ä»¶å
- **path** - å®Œæ•´è·¯å¾„
- **size** - æ–‡ä»¶å¤§å°ï¼ˆæœªå‹ç¼©ï¼‰
- **date** - ä¿®æ”¹æ—¥æœŸ (YYYY-MM-DD)
- **time** - ä¿®æ”¹æ—¶é—´ (HH:MM:SS)
- **ext** - çŸ­æ‰©å±•å (å¦‚ 'txt')
- **ext2** - é•¿æ‰©å±•å (å¦‚ 'tar.gz')
- **type** - file|dir|link
- **archive** - å‹ç¼©åŒ…ç±»å‹ (tar|zip|7z|rar)
- **container** - å®¹å™¨è·¯å¾„

## è¾…åŠ©å±æ€§

- **today** - ä»Šå¤©çš„æ—¥æœŸ
- **mo, tu, we, th, fr, sa, su** - ä¸Šä¸€ä¸ªå·¥ä½œæ—¥æ—¥æœŸ

## è¿ç®—ç¬¦

- **æ¯”è¾ƒ**: =, !=, <>, <, >, <=, >=
- **é€»è¾‘**: AND, OR, NOT
- **æ¨¡å¼**: LIKE, ILIKE (ä¸åŒºåˆ†å¤§å°å†™), RLIKE (æ­£åˆ™)
- **èŒƒå›´**: BETWEEN, IN
"""
        
        return FindzOutput(
            success=True,
            message="è¿‡æ»¤è¯­æ³•å¸®åŠ©",
            data={'help': help_text}
        )
